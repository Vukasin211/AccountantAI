import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import StandardScaler


# Load your dataset from the uploaded CSV
df = pd.read_csv("expenses.csv")  # replace with your filename

# Assume your dataset has a column called 'type' or 'transaction_type'
# that says 'income' or 'expense'
expenses = df[df['Type'] == 'Expense'].copy()

# Only keep the columns we need for LSTM: date and amount
expenses = expenses[['Date', 'Amount']]

# Optional: sort by date just in case
expenses['Date'] = pd.to_datetime(expenses['Date'])
expenses = expenses.sort_values('Date')
expenses.reset_index(drop=True, inplace=True)

# LSTM works better with scaled numbers (between 0 and 1)
scaler = StandardScaler()
expenses['amount_scaled'] = scaler.fit_transform(expenses[['Amount']])

# Define how many past points to look at
window_size = 15  # for example, last 5 expenses to predict next

X = []  # input sequences
y = []  # next value to predict

amounts = expenses['amount_scaled'].values

for i in range(window_size, len(amounts)):
    X.append(amounts[i-window_size:i])  # past 5 points
    y.append(amounts[i])                # next point

# Convert to numpy arrays
X = np.array(X)
y = np.array(y)

# Reshape X to 3D for LSTM: (samples, timesteps, features)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))
model.add(Dense(1))  # predict next amount
model.compile(optimizer='adam', loss='mse')  # mean squared error

# Train the model
history = model.fit(X, y, epochs=50, batch_size=8, verbose=1)

# Predict for all sequences
predictions = model.predict(X)

# Inverse transform to get original amounts
predicted_amounts = scaler.inverse_transform(predictions)
real_amounts = scaler.inverse_transform(y.reshape(-1, 1))

# Define your accuracy threshold (5% difference)
threshold = 0.10  # 5%

# Calculate absolute percentage difference for each prediction
diff_percentages = np.abs((real_amounts - predicted_amounts) / real_amounts)

# Count how many are within the threshold
correct_count = np.sum(diff_percentages <= threshold)
total_count = len(diff_percentages)

# Calculate percentage
accuracy_percent = (correct_count / total_count) * 100

print(f"‚úÖ Prediction accuracy: {accuracy_percent:.2f}% (within ¬±{threshold*100:.0f}% error margin)")


model.save("lstm_expense_model.h5")
# Later you can load it using: from tensorflow.keras.models import load_model

# Let's plot the last N predictions for clarity
N = 100  # number of data points to plot (adjust as you like)

plt.figure(figsize=(12, 6))
plt.plot(real_amounts[-N:], label='Real Expenses', color='blue')
plt.plot(predicted_amounts[-N:], label='Predicted Expenses', color='red', linestyle='dashed')

plt.title('Real vs Predicted Expenses')
plt.xlabel('Time Steps')
plt.ylabel('Expense Amount')
plt.legend()
plt.grid(True)
plt.show()

#Prvi nacin postize efikasnost od 13% u najboljem trenutnku
#Drugi model ispod postize efikasnost od 25%

# =======================
# 1Ô∏è‚É£ IMPORT LIBRARIES
# =======================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.models import load_model

# =======================
# 2Ô∏è‚É£ LOAD AND CLEAN DATA
# =======================

# Load your dataset from the uploaded CSV
df = pd.read_csv("expenses.csv")  # replace with your filename

# Filter only expense rows (ignore incomes)
expenses = df[df['Type'] == 'Expense'].copy()

# Keep only Date and Amount columns
expenses = expenses[['Date', 'Amount']]

# Sort by date and reset index
expenses['Date'] = pd.to_datetime(expenses['Date'])
expenses = expenses.sort_values('Date').reset_index(drop=True)

# =======================
# 3Ô∏è‚É£ SCALE DATA
# =======================
# LSTM works better with normalized/scaled data
scaler = StandardScaler()
expenses['amount_scaled'] = scaler.fit_transform(expenses[['Amount']])

# =======================
# 4Ô∏è‚É£ SPLIT INTO LOW/MID/HIGH EXPENSE RANGES
# =======================
# Calculate thresholds for different spending levels
low_th = expenses['Amount'].quantile(0.33)
high_th = expenses['Amount'].quantile(0.66)

# Create subsets for each spending level
low_data = expenses[expenses['Amount'] <= low_th]
mid_data = expenses[(expenses['Amount'] > low_th) & (expenses['Amount'] <= high_th)]
high_data = expenses[expenses['Amount'] > high_th]

# Helper function to prepare input/output data for an LSTM model
def prepare_sequences(data, window_size=15):
    amounts = data['amount_scaled'].values
    X, y = [], []
    for i in range(window_size, len(amounts)):
        X.append(amounts[i-window_size:i])
        y.append(amounts[i])
    X = np.array(X)
    y = np.array(y)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # reshape for LSTM
    return X, y

# =======================
# 5Ô∏è‚É£ DEFINE LSTM MODEL CREATION FUNCTION
# =======================
def create_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# =======================
# 6Ô∏è‚É£ TRAIN 3 SPECIALIZED MODELS
# =======================
models = {}
for label, dataset in zip(['low', 'mid', 'high'], [low_data, mid_data, high_data]):
    if len(dataset) > 30:  # make sure we have enough data
        X, y = prepare_sequences(dataset)
        model = create_lstm_model((X.shape[1], X.shape[2]))
        print(f"\nüöÄ Training {label.upper()} expense model...")
        model.fit(X, y, epochs=50, batch_size=8, verbose=1)
        models[label] = (model, X, y)
    else:
        print(f"‚ö†Ô∏è Not enough data for {label} range!")

# =======================
# 7Ô∏è‚É£ PREDICT WITH RULE-BASED SELECTOR
# =======================
# Combine data back in chronological order for prediction
X_all, y_all = prepare_sequences(expenses)
predictions_combined = []

# Predict for each data point using correct specialized model
for i in range(len(y_all)):
    # Determine which range the *previous real value* belongs to
    last_real_value = scaler.inverse_transform(y_all[i].reshape(-1, 1))[0][0]
    if last_real_value <= low_th and 'low' in models:
        model = models['low'][0]
    elif last_real_value <= high_th and 'mid' in models:
        model = models['mid'][0]
    elif 'high' in models:
        model = models['high'][0]
    else:
        continue

    pred = model.predict(X_all[i].reshape(1, X_all.shape[1], 1), verbose=0)
    predictions_combined.append(pred[0][0])

# =======================
# 8Ô∏è‚É£ CALCULATE ACCURACY AND ERROR
# =======================
predicted_amounts = scaler.inverse_transform(np.array(predictions_combined).reshape(-1, 1))
real_amounts = scaler.inverse_transform(y_all[:len(predicted_amounts)].reshape(-1, 1))

# Calculate % difference between predicted and real
diff_percentages = np.abs((real_amounts - predicted_amounts) / real_amounts)
threshold = 0.10  # 10% margin
correct_count = np.sum(diff_percentages <= threshold)
total_count = len(diff_percentages)
accuracy_percent = (correct_count / total_count) * 100

print(f"\n‚úÖ Overall multi-model accuracy: {accuracy_percent:.2f}% (within ¬±{threshold*100:.0f}% margin)")

# =======================
# 9Ô∏è‚É£ VISUALIZE RESULTS
# =======================
N = 100  # number of last data points to plot
plt.figure(figsize=(12, 6))
plt.plot(real_amounts[-N:], label='Real Expenses', color='blue')
plt.plot(predicted_amounts[-N:], label='Predicted Expenses', color='red', linestyle='dashed')
plt.title('Real vs Predicted Expenses (Multi-Model LSTM)')
plt.xlabel('Time Steps')
plt.ylabel('Expense Amount')
plt.legend()
plt.grid(True)
plt.show()

# =======================
# üîü SAVE MODELS
# =======================
for label, (model, _, _) in models.items():
    model.save(f"lstm_expense_model_{label}.h5")
    print(f"üíæ Saved model for {label} spending range.")

#najbolji model do sat 29% tacnosti
# =======================
# 1Ô∏è‚É£ IMPORT LIBRARIES
# =======================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense
import os

# =======================
# 2Ô∏è‚É£ LOAD AND CLEAN DATA
# =======================
df = pd.read_csv("expenses.csv")  # replace with your filename

# Filter only expenses
expenses = df[df['Type'] == 'Expense'].copy()
expenses = expenses[['Date', 'Amount']]
expenses['Date'] = pd.to_datetime(expenses['Date'])
expenses = expenses.sort_values('Date').reset_index(drop=True)

# =======================
# 3Ô∏è‚É£ SCALE DATA
# =======================
scaler = StandardScaler()
expenses['amount_scaled'] = scaler.fit_transform(expenses[['Amount']])

# =======================
# 4Ô∏è‚É£ SPLIT INTO LOW/MID/HIGH EXPENSE RANGES
# =======================
low_th = expenses['Amount'].quantile(0.33)
high_th = expenses['Amount'].quantile(0.66)

low_data = expenses[expenses['Amount'] <= low_th]
mid_data = expenses[(expenses['Amount'] > low_th) & (expenses['Amount'] <= high_th)]
high_data = expenses[expenses['Amount'] > high_th]

# =======================
# 5Ô∏è‚É£ HELPER FUNCTIONS
# =======================
def prepare_sequences(data, window_size=10):
    """Prepare sequences for LSTM"""
    amounts = data['amount_scaled'].values
    X, y = [], []
    for i in range(window_size, len(amounts)):
        X.append(amounts[i-window_size:i])
        y.append(amounts[i])
    X = np.array(X)
    y = np.array(y)
    X = X.reshape((X.shape[0], X.shape[1], 1))
    return X, y

def create_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def model_accuracy(y_true, y_pred, threshold=0.10):
    diff = np.abs((y_true - y_pred) / y_true)
    return np.sum(diff <= threshold) / len(diff) * 100

# =======================
# 6Ô∏è‚É£ TRAIN OR LOAD MODELS
# =======================
models = {}
window_size = 11
model_labels = ['low', 'mid', 'high']
data_dict = {'low': low_data, 'mid': mid_data, 'high': high_data}

for label in model_labels:
    model_file = f"lstm_expense_model_{label}.h5"
    dataset = data_dict[label]
    if os.path.exists(model_file):
        print(f"üìÇ Loading existing {label} model...")
        model = load_model(model_file, compile=False)
        X, y = prepare_sequences(dataset, window_size)
        models[label] = (model, X, y)
    else:
        if len(dataset) > window_size + 1:
            print(f"\nüöÄ Training new {label} model...")
            X, y = prepare_sequences(dataset, window_size)
            model = create_lstm_model((X.shape[1], X.shape[2]))
            model.fit(X, y, epochs=50, batch_size=8, verbose=1)
            model.save(model_file)
            models[label] = (model, X, y)
            print(f"üíæ Saved new {label} model.")
        else:
            print(f"‚ö†Ô∏è Not enough data to train {label} model!")

# =======================
# 7Ô∏è‚É£ PREDICTION
# =======================
X_all, y_all = prepare_sequences(expenses, window_size)
predictions_combined = []

for i in range(len(y_all)):
    last_real_value = scaler.inverse_transform(y_all[i].reshape(-1, 1))[0][0]
    if last_real_value <= low_th and 'low' in models:
        model = models['low'][0]
    elif last_real_value <= high_th and 'mid' in models:
        model = models['mid'][0]
    elif 'high' in models:
        model = models['high'][0]
    else:
        continue

    pred = model.predict(X_all[i].reshape(1, window_size, 1), verbose=0)
    predictions_combined.append(pred[0][0])

# =======================
# 8Ô∏è‚É£ CALCULATE ACCURACY
# =======================
predicted_amounts = scaler.inverse_transform(np.array(predictions_combined).reshape(-1, 1))
real_amounts = scaler.inverse_transform(y_all[:len(predicted_amounts)].reshape(-1, 1))

diff_percentages = np.abs((real_amounts - predicted_amounts) / real_amounts)
threshold = 0.10
correct_count = np.sum(diff_percentages <= threshold)
total_count = len(diff_percentages)
accuracy_percent = (correct_count / total_count) * 100

print(f"\n‚úÖ Overall multi-model accuracy: {accuracy_percent:.2f}% (¬±{threshold*100:.0f}% margin)")

# =======================
# 9Ô∏è‚É£ VISUALIZE RESULTS
# =======================
N = 100
plt.figure(figsize=(12, 6))
plt.plot(real_amounts[-N:], label='Real Expenses', color='blue')
plt.plot(predicted_amounts[-N:], label='Predicted Expenses', color='red', linestyle='dashed')
plt.title('Real vs Predicted Expenses (Multi-Model LSTM)')
plt.xlabel('Time Steps')
plt.ylabel('Expense Amount')
plt.legend()
plt.grid(True)
plt.show()

# =======================
# üîü SAVE MODELS
# =======================
for label, (model, _, _) in models.items():
    model.save(f"lstm_expense_model_{label}.h5")
    print(f"üíæ Saved model for {label} spending range.")

# =======================
# 1Ô∏è‚É£1Ô∏è‚É£ AUTO-REBALANCING SYSTEM
# =======================
print("\nüìä Checking accuracy per model before rebalancing...")
model_accuracies = {}

for label, (model, X, y) in models.items():
    preds = model.predict(X, verbose=0)
    preds_inv = scaler.inverse_transform(preds)
    y_inv = scaler.inverse_transform(y.reshape(-1,1))
    acc = model_accuracy(y_inv, preds_inv)
    model_accuracies[label] = acc
    print(f" - {label.capitalize()} model accuracy: {acc:.2f}%")

# Retrain models below threshold
retrain_threshold = 20
for label, acc in model_accuracies.items():
    if acc < retrain_threshold:
        print(f"\n‚öôÔ∏è Retraining '{label}' model (accuracy {acc:.1f}% < {retrain_threshold}%)...")
        _, X, y = models[label]
        new_model = create_lstm_model((X.shape[1], X.shape[2]))
        new_model.fit(X, y, epochs=150, batch_size=8, verbose=0)
        models[label] = (new_model, X, y)
        new_model.save(f"lstm_expense_model_{label}.h5")
        print(f"‚úÖ '{label}' model retrained and saved.")

# Optional: adjust thresholds if distribution shifts
expense_counts = {'low': len(low_data), 'mid': len(mid_data), 'high': len(high_data)}
total = sum(expense_counts.values())
ratios = {k: v / total for k, v in expense_counts.items()}

if ratios['high'] > 0.5:
    print("\nüìà Adjusting spending range thresholds (too many 'high' expenses)...")
    low_th = expenses['Amount'].quantile(0.25)
    high_th = expenses['Amount'].quantile(0.75)
    print(f"New thresholds: low ‚â§ {low_th:.2f}, high ‚â• {high_th:.2f}")

print("\n‚úÖ Auto-rebalancing complete.")

#Poslednje
# =======================
# 1Ô∏è‚É£ IMPORT LIBRARIES
# =======================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense
import os

# =======================
# 2Ô∏è‚É£ LOAD AND CLEAN DATA
# =======================
df = pd.read_csv("expenses.csv")  # replace with your filename

# Filter only expenses
expenses = df[df['Type'] == 'Expense'].copy()
expenses = expenses[['Date', 'Amount']]
expenses['Date'] = pd.to_datetime(expenses['Date'])
expenses = expenses.sort_values('Date').reset_index(drop=True)

# =======================
# 3Ô∏è‚É£ SCALE DATA
# =======================
scaler = StandardScaler()
expenses['amount_scaled'] = scaler.fit_transform(expenses[['Amount']])

# =======================
# 4Ô∏è‚É£ SPLIT INTO LOW/MID/HIGH EXPENSE RANGES
# =======================
low_th = expenses['Amount'].quantile(0.33)
high_th = expenses['Amount'].quantile(0.66)

low_data = expenses[expenses['Amount'] <= low_th]
mid_data = expenses[(expenses['Amount'] > low_th) & (expenses['Amount'] <= high_th)]
high_data = expenses[expenses['Amount'] > high_th]

# =======================
# 5Ô∏è‚É£ HELPER FUNCTIONS
# =======================
def prepare_sequences(data, window_size=10):
    """Prepare sequences for LSTM"""
    amounts = data['amount_scaled'].values
    X, y = [], []
    for i in range(window_size, len(amounts)):
        X.append(amounts[i-window_size:i])
        y.append(amounts[i])
    X = np.array(X)
    y = np.array(y)
    X = X.reshape((X.shape[0], X.shape[1], 1))
    return X, y

def create_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def model_accuracy(y_true, y_pred, threshold=0.10):
    diff = np.abs((y_true - y_pred) / y_true)
    return np.sum(diff <= threshold) / len(diff) * 100

# =======================
# 6Ô∏è‚É£ TRAIN OR LOAD MODELS
# =======================
models = {}
window_size = 11
model_labels = ['low', 'mid', 'high']
data_dict = {'low': low_data, 'mid': mid_data, 'high': high_data}

for label in model_labels:
    model_file = f"lstm_expense_model_{label}.h5"
    dataset = data_dict[label]
    if os.path.exists(model_file):
        print(f"üìÇ Loading existing {label} model...")
        model = load_model(model_file, compile=False)
        X, y = prepare_sequences(dataset, window_size)
        models[label] = (model, X, y)
    else:
        if len(dataset) > window_size + 1:
            print(f"\nüöÄ Training new {label} model...")
            X, y = prepare_sequences(dataset, window_size)
            model = create_lstm_model((X.shape[1], X.shape[2]))
            model.fit(X, y, epochs=50, batch_size=8, verbose=1)
            model.save(model_file)
            models[label] = (model, X, y)
            print(f"üíæ Saved new {label} model.")
        else:
            print(f"‚ö†Ô∏è Not enough data to train {label} model!")

# =======================
# 7Ô∏è‚É£ PREDICTION
# =======================
X_all, y_all = prepare_sequences(expenses, window_size)
predictions_combined = []

for i in range(len(y_all)):
    last_real_value = scaler.inverse_transform(y_all[i].reshape(-1, 1))[0][0]
    if last_real_value <= low_th and 'low' in models:
        model = models['low'][0]
    elif last_real_value <= high_th and 'mid' in models:
        model = models['mid'][0]
    elif 'high' in models:
        model = models['high'][0]
    else:
        continue

    pred = model.predict(X_all[i].reshape(1, window_size, 1), verbose=0)
    predictions_combined.append(pred[0][0])

# =======================
# 8Ô∏è‚É£ CALCULATE ACCURACY (¬±30% THRESHOLD)
# =======================
predicted_amounts = scaler.inverse_transform(np.array(predictions_combined).reshape(-1, 1))
real_amounts = scaler.inverse_transform(y_all[:len(predicted_amounts)].reshape(-1, 1))

threshold = 0.30  # 30% margin
correct_mask = np.abs(predicted_amounts - real_amounts) / real_amounts <= threshold

correct_count = np.sum(correct_mask)
total_count = len(correct_mask)
accuracy_percent = (correct_count / total_count) * 100

print(f"\n‚úÖ Overall accuracy (¬±{threshold*100:.0f}% margin): {accuracy_percent:.2f}%")

# =======================
# 9Ô∏è‚É£ VISUALIZE RESULTS (USE OVERALL ACCURACY FROM 8Ô∏è‚É£)
# =======================
N = 100  # number of last data points to plot

# Slice last N points for plotting lines
pred_slice = predicted_amounts[-N:]
real_slice = real_amounts[-N:]
correct_slice = correct_mask[-N:]  # still needed for shading

plt.figure(figsize=(12, 6))

# Plot real and predicted expenses
plt.plot(real_slice, label='Real Expenses', color='blue')
plt.plot(pred_slice, label='Predicted Expenses', color='red', linestyle='dashed')

# Shade background for correct/incorrect predictions
for i in range(N):
    if correct_slice[i]:
        plt.axvspan(i-0.5, i+0.5, color='green', alpha=0.1)  # correct
    else:
        plt.axvspan(i-0.5, i+0.5, color='red', alpha=0.1)    # outside ¬±30%

# Use overall accuracy from section 8
plt.text(0.02, 0.95, f'Overall Accuracy (¬±30%): {accuracy_percent:.2f}%', 
         transform=plt.gca().transAxes, fontsize=12, 
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))

plt.title('Real vs Predicted Expenses (¬±30% Correctness)')
plt.xlabel('Time Steps')
plt.ylabel('Expense Amount')
plt.legend()
plt.grid(True)
plt.show()

# =======================
# üîü SAVE MODELS
# =======================
for label, (model, _, _) in models.items():
    model.save(f"lstm_expense_model_{label}.h5")
    print(f"üíæ Saved model for {label} spending range.")

# =======================
# 1Ô∏è‚É£1Ô∏è‚É£ AUTO-REBALANCING SYSTEM
# =======================
print("\nüìä Checking accuracy per model before rebalancing...")
model_accuracies = {}

for label, (model, X, y) in models.items():
    preds = model.predict(X, verbose=0)
    preds_inv = scaler.inverse_transform(preds)
    y_inv = scaler.inverse_transform(y.reshape(-1,1))
    acc = model_accuracy(y_inv, preds_inv)
    model_accuracies[label] = acc
    print(f" - {label.capitalize()} model accuracy: {acc:.2f}%")

# Retrain models below threshold
retrain_threshold = 20
for label, acc in model_accuracies.items():
    if acc < retrain_threshold:
        print(f"\n‚öôÔ∏è Retraining '{label}' model (accuracy {acc:.1f}% < {retrain_threshold}%)...")
        _, X, y = models[label]
        new_model = create_lstm_model((X.shape[1], X.shape[2]))
        new_model.fit(X, y, epochs=150, batch_size=8, verbose=0)
        models[label] = (new_model, X, y)
        new_model.save(f"lstm_expense_model_{label}.h5")
        print(f"‚úÖ '{label}' model retrained and saved.")

# Optional: adjust thresholds if distribution shifts
expense_counts = {'low': len(low_data), 'mid': len(mid_data), 'high': len(high_data)}
total = sum(expense_counts.values())
ratios = {k: v / total for k, v in expense_counts.items()}

if ratios['high'] > 0.5:
    print("\nüìà Adjusting spending range thresholds (too many 'high' expenses)...")
    low_th = expenses['Amount'].quantile(0.25)
    high_th = expenses['Amount'].quantile(0.75)
    print(f"New thresholds: low ‚â§ {low_th:.2f}, high ‚â• {high_th:.2f}")

print("\n‚úÖ Auto-rebalancing complete.")

#najnovije




# =======================
# 1Ô∏è‚É£ IMPORT LIBRARIES
# =======================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense
import os

# =======================
# 2Ô∏è‚É£ LOAD AND CLEAN DATA
# =======================
df = pd.read_csv("Personal_Finance_Dataset.csv")  # replace with your filename

# Filter only expenses
expenses = df[df['Type'] == 'Expense'].copy()
expenses = expenses[['Date', 'Amount']]
expenses['Date'] = pd.to_datetime(expenses['Date'])
expenses = expenses.sort_values('Date').reset_index(drop=True)

# =======================
# 3Ô∏è‚É£ SCALE DATA
# =======================
scaler = StandardScaler()
expenses['amount_scaled'] = scaler.fit_transform(expenses[['Amount']])

# =======================
# 4Ô∏è‚É£ SPLIT INTO LOW/MID/HIGH EXPENSE RANGES
# =======================
low_th = expenses['Amount'].quantile(0.33)
high_th = expenses['Amount'].quantile(0.66)

low_data = expenses[expenses['Amount'] <= low_th]
mid_data = expenses[(expenses['Amount'] > low_th) & (expenses['Amount'] <= high_th)]
high_data = expenses[expenses['Amount'] > high_th]

# =======================
# 5Ô∏è‚É£ HELPER FUNCTIONS
# =======================
def prepare_sequences(data, window_size=10):
    """Prepare sequences for LSTM"""
    amounts = data['amount_scaled'].values
    X, y = [], []
    for i in range(window_size, len(amounts)):
        X.append(amounts[i-window_size:i])
        y.append(amounts[i])
    X = np.array(X)
    y = np.array(y)
    X = X.reshape((X.shape[0], X.shape[1], 1))
    return X, y

def create_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def model_accuracy(y_true, y_pred, threshold=0.10):
    diff = np.abs((y_true - y_pred) / y_true)
    return np.sum(diff <= threshold) / len(diff) * 100

def calculate_all_metrics(y_true, y_pred):
    """Calculate comprehensive error metrics"""
    # Basic errors
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    
    # Percentage errors
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    mpe = np.mean((y_true - y_pred) / y_true) * 100  # Mean Percentage Error
    
    # Relative errors
    relative_errors = np.abs(y_true - y_pred) / y_true
    median_ape = np.median(np.abs((y_true - y_pred) / y_true)) * 100
    
    # Statistical errors
    max_error = np.max(np.abs(y_true - y_pred))
    mean_error = np.mean(y_true - y_pred)
    std_error = np.std(y_true - y_pred)
    
    # Accuracy at different thresholds
    acc_10 = model_accuracy(y_true, y_pred, 0.10)
    acc_20 = model_accuracy(y_true, y_pred, 0.20)
    acc_30 = model_accuracy(y_true, y_pred, 0.30)
    
    # R-squared
    r2 = r2_score(y_true, y_pred)
    
    return {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'MAPE': mape,
        'MPE': mpe,
        'Median_APE': median_ape,
        'Max_Error': max_error,
        'Mean_Error': mean_error,
        'Std_Error': std_error,
        'Accuracy_10%': acc_10,
        'Accuracy_20%': acc_20,
        'Accuracy_30%': acc_30,
        'R2_Score': r2
    }

def print_metrics(metrics, title="Error Metrics"):
    """Print formatted error metrics"""
    print(f"\nüìä {title}:")
    print("=" * 50)
    
    # Basic errors
    print("üìç BASIC ERRORS:")
    print(f"   - Mean Absolute Error (MAE): ${metrics['MAE']:.2f}")
    print(f"   - Root Mean Square Error (RMSE): ${metrics['RMSE']:.2f}")
    print(f"   - Mean Squared Error (MSE): ${metrics['MSE']:.2f}")
    print(f"   - Max Error: ${metrics['Max_Error']:.2f}")
    
    # Percentage errors
    print("\nüìç PERCENTAGE ERRORS:")
    print(f"   - Mean Absolute Percentage Error (MAPE): {metrics['MAPE']:.2f}%")
    print(f"   - Median Absolute Percentage Error: {metrics['Median_APE']:.2f}%")
    print(f"   - Mean Percentage Error (MPE): {metrics['MPE']:.2f}%")
    
    # Statistical errors
    print("\nüìç STATISTICAL ERRORS:")
    print(f"   - Mean Error (Bias): ${metrics['Mean_Error']:.2f}")
    print(f"   - Standard Deviation of Errors: ${metrics['Std_Error']:.2f}")
    
    # Accuracy metrics
    print("\nüìç ACCURACY METRICS:")
    print(f"   - Accuracy (¬±10%): {metrics['Accuracy_10%']:.2f}%")
    print(f"   - Accuracy (¬±20%): {metrics['Accuracy_20%']:.2f}%")
    print(f"   - Accuracy (¬±30%): {metrics['Accuracy_30%']:.2f}%")
    print(f"   - R¬≤ Score: {metrics['R2_Score']:.4f}")
    
    # Interpretation
    print("\nüìç INTERPRETATION:")
    if metrics['Mean_Error'] > 0:
        print(f"   - Model tends to UNDERESTIMATE expenses by ${metrics['Mean_Error']:.2f} on average")
    else:
        print(f"   - Model tends to OVERESTIMATE expenses by ${-metrics['Mean_Error']:.2f} on average")
    
    if metrics['R2_Score'] > 0.8:
        print("   - Excellent predictive power (R¬≤ > 0.8)")
    elif metrics['R2_Score'] > 0.6:
        print("   - Good predictive power (R¬≤ > 0.6)")
    elif metrics['R2_Score'] > 0.4:
        print("   - Moderate predictive power (R¬≤ > 0.4)")
    else:
        print("   - Weak predictive power (R¬≤ ‚â§ 0.4)")

# =======================
# 6Ô∏è‚É£ TRAIN OR LOAD MODELS
# =======================
models = {}
window_size = 11
model_labels = ['low', 'mid', 'high']
data_dict = {'low': low_data, 'mid': mid_data, 'high': high_data}

for label in model_labels:
    model_file = f"lstm_expense_model_{label}.h5"
    dataset = data_dict[label]
    if os.path.exists(model_file):
        print(f"üìÇ Loading existing {label} model...")
        model = load_model(model_file, compile=False)
        X, y = prepare_sequences(dataset, window_size)
        models[label] = (model, X, y)
    else:
        if len(dataset) > window_size + 1:
            print(f"\nüöÄ Training new {label} model...")
            X, y = prepare_sequences(dataset, window_size)
            model = create_lstm_model((X.shape[1], X.shape[2]))
            model.fit(X, y, epochs=50, batch_size=8, verbose=1)
            model.save(model_file)
            models[label] = (model, X, y)
            print(f"üíæ Saved new {label} model.")
        else:
            print(f"‚ö†Ô∏è Not enough data to train {label} model!")

# =======================
# 7Ô∏è‚É£ PREDICTION
# =======================
X_all, y_all = prepare_sequences(expenses, window_size)
predictions_combined = []

for i in range(len(y_all)):
    last_real_value = scaler.inverse_transform(y_all[i].reshape(-1, 1))[0][0]
    if last_real_value <= low_th and 'low' in models:
        model = models['low'][0]
    elif last_real_value <= high_th and 'mid' in models:
        model = models['mid'][0]
    elif 'high' in models:
        model = models['high'][0]
    else:
        continue

    pred = model.predict(X_all[i].reshape(1, window_size, 1), verbose=0)
    predictions_combined.append(pred[0][0])

# =======================
# 8Ô∏è‚É£ COMPREHENSIVE ERROR ANALYSIS
# =======================
predicted_amounts = scaler.inverse_transform(np.array(predictions_combined).reshape(-1, 1))
real_amounts = scaler.inverse_transform(y_all[:len(predicted_amounts)].reshape(-1, 1))

# Calculate all error metrics
overall_metrics = calculate_all_metrics(real_amounts, predicted_amounts)

# Print comprehensive metrics
print_metrics(overall_metrics, "OVERALL MODEL PERFORMANCE")

# Additional threshold analysis
threshold = 0.30
correct_mask = np.abs(predicted_amounts - real_amounts) / real_amounts <= threshold
correct_count = np.sum(correct_mask)
total_count = len(correct_mask)
accuracy_percent = (correct_count / total_count) * 100

print(f"\nüéØ KEY PERFORMANCE INDICATOR:")
print(f"   - Overall accuracy (¬±{threshold*100:.0f}% margin): {accuracy_percent:.2f}%")
print(f"   - Correct predictions: {correct_count}/{total_count}")

# =======================
# 9Ô∏è‚É£ VISUALIZE RESULTS WITH ERROR ANALYSIS
# =======================
N = min(100, len(real_amounts))  # number of last data points to plot

# Slice last N points for plotting lines
pred_slice = predicted_amounts[-N:]
real_slice = real_amounts[-N:]
correct_slice = correct_mask[-N:]

# Create subplots for better visualization
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))

# Plot 1: Real vs Predicted
ax1.plot(real_slice, label='Real Expenses', color='blue', linewidth=2)
ax1.plot(pred_slice, label='Predicted Expenses', color='red', linestyle='dashed', linewidth=2)

# Shade background for correct/incorrect predictions
for i in range(N):
    if correct_slice[i]:
        ax1.axvspan(i-0.5, i+0.5, color='green', alpha=0.1)
    else:
        ax1.axvspan(i-0.5, i+0.5, color='red', alpha=0.1)

ax1.text(0.02, 0.95, f'Overall Accuracy (¬±30%): {accuracy_percent:.2f}%\nMAE: ${overall_metrics["MAE"]:.2f}\nMAPE: {overall_metrics["MAPE"]:.2f}%', 
         transform=ax1.transAxes, fontsize=11, 
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

ax1.set_title('Real vs Predicted Expenses with Error Analysis')
ax1.set_ylabel('Expense Amount ($)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Prediction Errors
errors = real_slice - pred_slice
ax2.bar(range(N), errors.flatten(), color=np.where(errors.flatten() > 0, 'blue', 'red'), alpha=0.6)
ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)
ax2.axhline(y=overall_metrics['MAE'], color='orange', linestyle='--', linewidth=1, label=f'MAE (${overall_metrics["MAE"]:.2f})')
ax2.axhline(y=-overall_metrics['MAE'], color='orange', linestyle='--', linewidth=1)
ax2.set_xlabel('Time Steps')
ax2.set_ylabel('Prediction Error ($)')
ax2.set_title('Prediction Errors (Positive = Underestimation, Negative = Overestimation)')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# =======================
# üîü MODEL-SPECIFIC ERROR ANALYSIS
# =======================
print("\nüîç MODEL-SPECIFIC PERFORMANCE ANALYSIS:")
print("=" * 50)

model_specific_metrics = {}

for label, (model, X, y) in models.items():
    # Get predictions for this model
    preds = model.predict(X, verbose=0)
    preds_inv = scaler.inverse_transform(preds)
    y_inv = scaler.inverse_transform(y.reshape(-1, 1))
    
    # Calculate metrics
    metrics = calculate_all_metrics(y_inv, preds_inv)
    model_specific_metrics[label] = metrics
    
    print(f"\nüìà {label.upper()} MODEL:")
    print(f"   - Samples: {len(y_inv)}")
    print(f"   - MAE: ${metrics['MAE']:.2f}")
    print(f"   - MAPE: {metrics['MAPE']:.2f}%")
    print(f"   - Accuracy (¬±30%): {metrics['Accuracy_30%']:.2f}%")
    print(f"   - R¬≤ Score: {metrics['R2_Score']:.4f}")

# =======================
# 1Ô∏è‚É£1Ô∏è‚É£ SAVE MODELS
# =======================
for label, (model, _, _) in models.items():
    model.save(f"lstm_expense_model_{label}.h5")
    print(f"üíæ Saved model for {label} spending range.")

# =======================
# 1Ô∏è‚É£2Ô∏è‚É£ AUTO-REBALANCING SYSTEM WITH ENHANCED METRICS
# =======================
print("\nüìä AUTO-REBALANCING SYSTEM:")
print("=" * 50)

model_accuracies = {}
retrain_threshold = 20

for label, (model, X, y) in models.items():
    preds = model.predict(X, verbose=0)
    preds_inv = scaler.inverse_transform(preds)
    y_inv = scaler.inverse_transform(y.reshape(-1,1))
    acc = model_accuracy(y_inv, preds_inv)
    model_accuracies[label] = acc
    
    metrics = calculate_all_metrics(y_inv, preds_inv)
    print(f"\nüîß {label.capitalize()} model pre-rebalancing:")
    print(f"   - Accuracy: {acc:.2f}%")
    print(f"   - MAE: ${metrics['MAE']:.2f}")
    print(f"   - MAPE: {metrics['MAPE']:.2f}%")

# Retrain models below threshold
for label, acc in model_accuracies.items():
    if acc < retrain_threshold:
        print(f"\n‚öôÔ∏è Retraining '{label}' model (accuracy {acc:.1f}% < {retrain_threshold}%)...")
        _, X, y = models[label]
        new_model = create_lstm_model((X.shape[1], X.shape[2]))
        new_model.fit(X, y, epochs=150, batch_size=8, verbose=0)
        models[label] = (new_model, X, y)
        new_model.save(f"lstm_expense_model_{label}.h5")
        
        # Verify improvement
        preds = new_model.predict(X, verbose=0)
        preds_inv = scaler.inverse_transform(preds)
        y_inv = scaler.inverse_transform(y.reshape(-1,1))
        new_acc = model_accuracy(y_inv, preds_inv)
        print(f"‚úÖ '{label}' model retrained. New accuracy: {new_acc:.1f}%")

# Optional: adjust thresholds if distribution shifts
expense_counts = {'low': len(low_data), 'mid': len(mid_data), 'high': len(high_data)}
total = sum(expense_counts.values())
ratios = {k: v / total for k, v in expense_counts.items()}

print(f"\nüìà EXPENSE DISTRIBUTION:")
for label, ratio in ratios.items():
    print(f"   - {label.capitalize()}: {ratio:.1%} ({expense_counts[label]} samples)")

if ratios['high'] > 0.5:
    print("\nüìä Adjusting spending range thresholds (too many 'high' expenses)...")
    low_th = expenses['Amount'].quantile(0.25)
    high_th = expenses['Amount'].quantile(0.75)
    print(f"New thresholds: low ‚â§ {low_th:.2f}, high ‚â• {high_th:.2f}")

print("\n‚úÖ Auto-rebalancing complete with comprehensive error analysis!")